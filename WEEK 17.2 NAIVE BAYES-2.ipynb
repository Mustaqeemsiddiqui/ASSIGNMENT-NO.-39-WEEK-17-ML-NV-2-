{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7769a286-3d61-44b4-962d-775059c27521",
   "metadata": {},
   "source": [
    "**Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86332d88-9fd7-4686-a73a-b4bb79c34716",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we need to use conditional probability.\n",
    "\n",
    "Let's denote:\n",
    "- \\( P(S) \\) as the probability that an employee is a smoker.\n",
    "- \\( P(H) \\) as the probability that an employee uses the health insurance plan.\n",
    "- \\( P(S \\mid H) \\) as the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "From the problem statement:\n",
    "- \\( P(H) = 0.70 \\), since 70% of the employees use the health insurance plan.\n",
    "- \\( P(S \\mid H) = 0.40 \\), since 40% of the employees who use the health insurance plan are smokers.\n",
    "\n",
    "Now, using the definition of conditional probability:\n",
    "\\[ P(S \\mid H) = \\frac{P(S \\cap H)}{P(H)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(S \\cap H) \\) is the probability that an employee is both a smoker and uses the health insurance plan.\n",
    "\n",
    "We can find \\( P(S \\cap H) \\) using:\n",
    "\\[ P(S \\cap H) = P(S \\mid H) \\cdot P(H) \\]\n",
    "\n",
    "Substituting the values we know:\n",
    "\\[ P(S \\cap H) = 0.40 \\cdot 0.70 = 0.28 \\]\n",
    "\n",
    "Now, calculate \\( P(S \\mid H) \\):\n",
    "\\[ P(S \\mid H) = \\frac{0.28}{0.70} = \\frac{4}{10} = 0.4 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is \\( \\boxed{0.4} \\), or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e90fa-7987-439c-b02d-6dbf11b64a3b",
   "metadata": {},
   "source": [
    "**Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdc806-5c3c-4156-98cb-59490ac1a05b",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes (BNB) and Multinomial Naive Bayes (MNB) lies in how they model the feature probabilities:\n",
    "\n",
    "1. **Bernoulli Naive Bayes (BNB)**:\n",
    "   - BNB is typically used when features are binary (i.e., presence or absence of a feature).\n",
    "   - It assumes that features are binary variables, where each feature is considered independently and contributes equally to the likelihood regardless of the frequency of its occurrence.\n",
    "   - Example: Text classification where each term's presence or absence (whether a term appears in the document or not) is considered.\n",
    "\n",
    "2. **Multinomial Naive Bayes (MNB)**:\n",
    "   - MNB is used when features represent counts or frequencies of events (e.g., word counts in document classification).\n",
    "   - It assumes that features follow a multinomial distribution (which is a generalization of the binomial distribution for more than two categories).\n",
    "   - Example: Document classification based on word counts, where the frequency of each term in the document matters.\n",
    "\n",
    "In summary:\n",
    "- **BNB** deals with binary feature vectors, where the presence or absence of each feature is the focus.\n",
    "- **MNB** deals with count-based feature vectors, where the frequency of each feature (e.g., word counts) is considered.\n",
    "\n",
    "Both BNB and MNB are variations of the Naive Bayes classifier, which assumes independence between features given the class label (hence \"naive\"). They are commonly used in text classification and other tasks involving categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8609948-ea3a-4ee6-a3c3-0b04d46c2fab",
   "metadata": {},
   "source": [
    "**Q3. How does Bernoulli Naive Bayes handle missing values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804b0e3-9d0c-46d3-9ea6-05f401e171e4",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "Bernoulli Naive Bayes (BNB) typically handles missing values by treating them as a separate category or as an indication of absence, depending on how the model is trained and implemented. Here are common approaches to deal with missing values in the context of BNB:\n",
    "\n",
    "1. **Imputation as Absence (0)**:\n",
    "   - In many implementations, missing values are often treated as if the feature is absent (i.e., the binary value for that feature is set to 0).\n",
    "   - This approach assumes that the absence of information implies the feature is not present, aligning with the binary nature of Bernoulli Naive Bayes where features are either present (1) or absent (0).\n",
    "\n",
    "2. **Separate Category for Missing Values**:\n",
    "   - Another approach is to explicitly consider missing values as a separate category or state of the feature.\n",
    "   - This involves modifying the feature representation to include an additional category that explicitly denotes missing values.\n",
    "   - During training, the model learns how to classify instances with missing values based on the available information from other features.\n",
    "\n",
    "3. **Ignoring Missing Values**:\n",
    "   - In some implementations, missing values might be ignored during training, assuming their impact on classification is minimal or that their absence can be inferred indirectly through other features.\n",
    "   - However, this approach requires careful consideration of how missing values might affect classification performance and model accuracy.\n",
    "\n",
    "The specific handling of missing values can depend on the implementation details of the Bernoulli Naive Bayes algorithm in a particular software library or framework. It's important to review the documentation or specific implementation details to understand how missing values are treated in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f21fc6-b8af-4105-af68-f9fa39746855",
   "metadata": {},
   "source": [
    "**Q4. Can Gaussian Naive Bayes be used for multi-class classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af3bae-253c-43d5-8903-7716481eaec4",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "Yes, Gaussian Naive Bayes (GNB) can be used for multi-class classification tasks. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes continuous-valued features follow a Gaussian (normal) distribution. It's particularly suitable when dealing with continuous data where each class's features are normally distributed.\n",
    "\n",
    "Here's how Gaussian Naive Bayes handles multi-class classification:\n",
    "\n",
    "1. **Modeling Class Conditional Distributions**:\n",
    "   - For each class \\( C_k \\), GNB models the distribution of feature values as Gaussian (normal) distributions with mean \\( \\mu_{k,i} \\) and variance \\( \\sigma_{k,i}^2 \\) for each feature \\( i \\).\n",
    "\n",
    "2. **Class Prior Probability**:\n",
    "   - GNB calculates the prior probability \\( P(C_k) \\) for each class \\( C_k \\) based on the relative frequency of each class in the training data.\n",
    "\n",
    "3. **Posterior Probability Calculation**:\n",
    "   - To classify a new instance with feature vector \\( \\mathbf{x} = (x_1, x_2, \\ldots, x_n) \\):\n",
    "     \\[ P(C_k \\mid \\mathbf{x}) \\propto P(C_k) \\prod_{i=1}^{n} P(x_i \\mid C_k) \\]\n",
    "   - \\( P(x_i \\mid C_k) \\) is computed using the Gaussian probability density function with parameters \\( \\mu_{k,i} \\) and \\( \\sigma_{k,i}^2 \\).\n",
    "\n",
    "4. **Decision Rule**:\n",
    "   - The class \\( C_k \\) that maximizes \\( P(C_k \\mid \\mathbf{x}) \\) is chosen as the predicted class for the instance \\( \\mathbf{x} \\).\n",
    "\n",
    "Therefore, Gaussian Naive Bayes can handle multiple classes by extending the binary classification approach of Naive Bayes to accommodate multiple class labels. Each class is modeled with its own set of Gaussian distributions for the features, and classification is performed based on the most probable class given the observed feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb1d0a-756d-4344-aa10-9b43796cfdde",
   "metadata": {},
   "source": [
    "**Q5. Assignment:**\n",
    "\n",
    "\n",
    "**Data preparation:**\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "**Results:**\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "**Conclusion:**\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff724954-e246-4de8-9e47-4d44a7b8fd35",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "To proceed with implementing and evaluating the Naive Bayes classifiers on the Spambase dataset, we'll follow these steps:\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "1. **Download the Dataset**: Obtain the Spambase dataset from the UCI Machine Learning Repository.\n",
    "2. **Load the Dataset**: Read the dataset into a pandas DataFrame and prepare it for training and evaluation.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "3. **Implement Naive Bayes Classifiers**:\n",
    "   - Bernoulli Naive Bayes\n",
    "   - Multinomial Naive Bayes\n",
    "   - Gaussian Naive Bayes\n",
    "\n",
    "4. **Use scikit-learn Library**: Utilize scikit-learn's implementations of these classifiers.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "5. **Perform 10-fold Cross-Validation**: Evaluate each classifier using 10-fold cross-validation to obtain robust performance metrics.\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "6. **Report Metrics**: Compute and report the following metrics for each classifier:\n",
    "   - Accuracy\n",
    "   - Precision\n",
    "   - Recall\n",
    "   - F1 score\n",
    "\n",
    "### Discussion\n",
    "\n",
    "7. **Discuss Results**: Analyze and compare the performance of the classifiers. Discuss which variant of Naive Bayes performed the best and why. Highlight any limitations observed during the evaluation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "8. **Summarize Findings**: Provide a summary of the findings and suggest potential areas for future work or improvements.\n",
    "\n",
    "Let's begin by downloading the dataset and implementing the classifiers in Python using scikit-learn.\n",
    "\n",
    "Hereâ€™s how you can proceed with implementing and evaluating the Naive Bayes classifiers on the Spambase dataset using Python and scikit-learn:\n",
    "\n",
    "### Step 1: Data Preparation\n",
    "\n",
    "Download the Spambase dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Spambase). Save the dataset file (`spambase.data`) in your working directory.\n",
    "\n",
    "### Step 2: Implementation\n",
    "\n",
    "Let's implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246fafe2-d628-42f0-9476-abf91bdd9a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "Classifier: Multinomial Naive Bayes\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "Classifier: Gaussian Naive Bayes\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\",\n",
    "    \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n",
    "    \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\", \"word_freq_cs\",\n",
    "    \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\",\n",
    "    \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\",\n",
    "    \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\", \"spam\"\n",
    "]\n",
    "data = pd.read_csv(url, names=names, header=None)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('spam', axis=1)\n",
    "y = data['spam']\n",
    "\n",
    "# Initialize classifiers\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Initialize lists to store results\n",
    "classifiers = [bnb, mnb, gnb]\n",
    "clf_names = ['Bernoulli Naive Bayes', 'Multinomial Naive Bayes', 'Gaussian Naive Bayes']\n",
    "\n",
    "# Evaluate each classifier using 10-fold cross-validation\n",
    "for clf, clf_name in zip(classifiers, clf_names):\n",
    "    scores_accuracy = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "    scores_precision = cross_val_score(clf, X, y, cv=10, scoring='precision')\n",
    "    scores_recall = cross_val_score(clf, X, y, cv=10, scoring='recall')\n",
    "    scores_f1 = cross_val_score(clf, X, y, cv=10, scoring='f1')\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Classifier: {clf_name}\")\n",
    "    print(f\"Accuracy: {scores_accuracy.mean():.4f}\")\n",
    "    print(f\"Precision: {scores_precision.mean():.4f}\")\n",
    "    print(f\"Recall: {scores_recall.mean():.4f}\")\n",
    "    print(f\"F1 Score: {scores_f1.mean():.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86df76b-7792-46a3-be62-738b64a8b5da",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "After running the above code, you will get the performance metrics (accuracy, precision, recall, F1 score) for each variant of Naive Bayes. Analyze the results to compare which classifier performed the best and discuss the reasons behind their performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Summarize your findings based on the results obtained. Discuss any limitations observed with Naive Bayes classifiers and suggest potential future work or improvements.\n",
    "\n",
    "This approach will provide you with a comprehensive evaluation of Naive Bayes classifiers on the Spambase dataset, demonstrating their performance on a real-world binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67d336-497d-45b2-a6c0-3b1b87471046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ad92a-c969-4a2e-ac1f-4084071fcb49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007188dc-88d8-432b-a33c-76d4303c3c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c52ea-d164-454b-b856-36a122cff2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47733287-1da4-4b37-9319-c0258b06684d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
